{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92bbdc46",
   "metadata": {},
   "source": [
    "### Brazilian E-Commerce Public Dataset by Olist\n",
    "This is a Brazilian ecommerce public dataset of orders made at Olist Store. The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers. We also released a geolocation dataset that relates Brazilian zip codes to lat/lng coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197dfa4",
   "metadata": {},
   "source": [
    "--- OLIST E-COMMERCE DATASET: ML FEATURE ARCHITECTURE REPORT ---\n",
    "\n",
    "The goal of this pre-processing pipeline was to establish a high-integrity, predictive feature space by prioritizing variance control, temporal integrity, and domain-aware imputation over simple data deletion.\n",
    "\n",
    "### 1. CUSTOMERS & SELLERS DATA (Geospatial Integrity)\n",
    "\n",
    "- ACTION: Applied rigorous standardization (lower/strip) to all `city` and `state` columns.\n",
    "- ML RATIONALE: Ensures uniformity for future geospatial feature engineering (e.g., embeddings) and prevents subtle textual variations from inflating categorical cardinality.\n",
    "- OUTLIER CHECK: Confirmed NO statistical outliers in Zip Code Prefixes (ZIPs are categorical identifiers, not continuous features).\n",
    "- OUTCOME: Clean, linkable demographic/location data for Customer Lifetime Value (CLV) and Logistics Models.\n",
    "\n",
    "### 2. ORDERS DATA (Temporal Causality & Target Population)\n",
    "\n",
    "- ACTION: Filtered dataset to retain only orders with `order_status` == 'delivered'.\n",
    "- ML RATIONALE: Establishes the necessary 'Ground Truth' for supervised models (delivery time, satisfaction). Incomplete orders are noise for these tasks.\n",
    "- ACTION: Converted all 5 timestamp columns to datetime objects and performed **TARGETED NULL DROPPING** on critical internal dates (`order_approved_at`, `order_delivered_carrier_date`).\n",
    "- ACTION: Explicitly removed orders violating temporal causality (e.g., approval before purchase, delivery before carrier).\n",
    "- ML RATIONALE: This crucial step protects the integrity of all derived time-difference features (like 'Delivery Lead Time') from non-physical errors.\n",
    "- OUTCOME: A temporally consistent central hub table for high-fidelity feature joining.\n",
    "\n",
    "### 3. ORDER ITEMS DATA (The Skewness Problem)\n",
    "\n",
    "- OUTLIER DETECTED: Extreme right-skewed outliers in `price` and `freight_value` (the 'Whale Orders').\n",
    "- ACTION: Applied **Logarithmic Transformation** ($\\\\log(1+x)$) to create `price_log` and `freight_value_log`.\n",
    "- ML RATIONALE: This transformation is chosen over deletion because the extreme values are REAL and predictive. Log-transform stabilizes variance, normalizes the distribution, and ensures compatibility with linear algorithms without sacrificing the influence of high-value signal.\n",
    "\n",
    "### 4. PRODUCTS DATA (Dimensionality and Attribute Robustness)\n",
    "\n",
    "- FEATURE CREATION: Explicitly derived the feature **`product_volume_cm3`** (Length * Height * Width) as a primary predictor for freight cost.\n",
    "- IMPUTATION STRATEGY: Imputed all missing physical dimensions/counts using the **MEDIAN**.\n",
    "- ML RATIONALE: The median is robust against the sparse outliers common in product dimensions, preserving the central tendency for logistics modeling.\n",
    "- OUTLIER CONTROL (Weight/Volume): Applied **Logarithmic Transformation** to `product_weight_g` and the newly created `product_volume_cm3` for variance stabilization (similar to Order Items).\n",
    "- OUTLIER CONTROL (Description): Applied **99th Percentile Capping (Winsorizing)** to `product_description_length`.\n",
    "- ML RATIONALE: This is a targeted strategy to limit the influence of the few extremely verbose descriptions on numerical proxies for text complexity.\n",
    "\n",
    "### 5. ORDER REVIEWS DATA (Sentiment Signal Integrity)\n",
    "\n",
    "- IMPUTATION STRATEGY: Filled the ~88% nulls in `review_comment_message` with the **'no comment'** string.\n",
    "- ML RATIONALE: A missing comment is a **meaningful, sentiment-neutral categorical class** for future NLP and satisfaction models.\n",
    "- OUTLIER DECISION (Scores): Statistical outliers (low scores of 1 or 2) were **RETAINED**.\n",
    "- ML RATIONALE: These are high-leverage, negative feedback data points essential for training **Churn and Customer Satisfaction Models**. Removing them would introduce positive bias.\n",
    "- INTEGRITY: Removed 814 duplicate `review_id` values to ensure clean 1:1 join with the central Orders table.\n",
    "\n",
    "### 6. PAYMENTS DATA (Financial Variance Control)\n",
    "\n",
    "- OUTLIER DETECTED: Extreme right-skew in `payment_value`.\n",
    "- ACTION: Applied **99.9th Percentile Capping (Winsorizing)** to create `capped_payment_value`.\n",
    "- ML RATIONALE: Capping is superior to deletion/log-transform here; it directly limits the influence of 'Whale' transactions on **L2 loss (MSE)**, stabilizing model training without removing the critical high-value signal.\n",
    "- ZERO-VALUE VALIDATION: Confirmed $0.00 payments are associated with the **'voucher'** payment type. Retained as a definitive, predictive categorical signal.\n",
    "\n",
    "### 7. GEOLOCATION DATA (Efficiency and Geographic Purity)\n",
    "\n",
    "- DATA REDUCTION: Dropped over 300,000 redundant duplicate coordinate mappings (on `zip_code_prefix`, `lat`, `lng`).\n",
    "- ML RATIONALE: Drastically improves memory footprint and join efficiency for a massive lookup table.\n",
    "- OUTLIER CHECK: Implemented a **Geographic Boundary Check** to remove coordinates falling outside the plausible bounds of Brazil.\n",
    "- ML RATIONALE: Ensures that the distance metrics derived from this table are based on physically accurate, valid seller and customer locations.\n",
    "\n",
    "### 8. TRANSLATION DATA (Interpretability Layer)\n",
    "\n",
    "- ACTION: No cleaning/transformation required (perfect quality).\n",
    "- ML RATIONALE: The file's sole purpose is to serve as a high-integrity lookup table, providing the essential **English language interpretability layer** for all categorical product features.\n",
    "\n",
    "---\n",
    "**FINAL STATUS: All nine relational data sources have been transformed into a dense, high-quality, and variance-stabilized feature matrix, ready for sophisticated EDA, feature engineering (e.g., distance and time-series metrics), and predictive modeling.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e252f0",
   "metadata": {},
   "source": [
    "The code in the following cell downloads the dataset from Kagglehub and lists the files in the downloaded directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Download the latest version of the dataset\n",
    "path = kagglehub.dataset_download(\"olistbr/brazilian-ecommerce\")\n",
    "\n",
    "# List the files in the downloaded directory\n",
    "file_list = os.listdir(path)\n",
    "for file_name in file_list:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_plot_outliers_iqr(df, column, exclude_zero=False):\n",
    "    \"\"\"\n",
    "    Detects outliers using the IQR method, prints outlier information, and plots a box plot.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): The name of the column to analyze for outliers.\n",
    "        exclude_zero (bool): Whether to exclude zero values from outlier calculation.\n",
    "                             Useful for columns where zero is a meaningful non-outlier value.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Outlier Analysis for column: {column} ---\")\n",
    "\n",
    "    # Create box plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(y=df[column])\n",
    "    plt.title(f'Box plot of {column}')\n",
    "    plt.ylabel(column)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate IQR and identify outliers\n",
    "    if exclude_zero:\n",
    "        # Consider only non-zero values for IQR calculation\n",
    "        data_for_iqr = df[df[column] != 0][column]\n",
    "    else:\n",
    "        data_for_iqr = df[column]\n",
    "\n",
    "    if data_for_iqr.empty:\n",
    "        print(f\"  No non-zero data in column '{column}' to calculate outliers.\")\n",
    "        return\n",
    "\n",
    "    Q1 = data_for_iqr.quantile(0.25)\n",
    "    Q3 = data_for_iqr.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify outliers in the original DataFrame\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "\n",
    "    num_outliers = len(outliers)\n",
    "    percentage_outliers = (num_outliers / len(df)) * 100\n",
    "\n",
    "    print(f\"  Number of outliers: {num_outliers}\")\n",
    "    print(f\"  Percentage of outliers: {percentage_outliers:.2f}%\")\n",
    "    print(f\"  Lower bound (IQR): {lower_bound:.2f}\")\n",
    "    print(f\"  Upper bound (IQR): {upper_bound:.2f}\")\n",
    "\n",
    "    if num_outliers > 0 and num_outliers < 20: # Display if not too many outliers\n",
    "        print(\"\\nSample Outlier Rows:\")\n",
    "        display(outliers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5157b26",
   "metadata": {},
   "source": [
    "### Processing the `olist_customers_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215fbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the customers dataset\n",
    "customers_dataset_path = os.path.join(path, \"olist_customers_dataset.csv\")\n",
    "customers_df = pd.read_csv(customers_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the customers_df DataFrame\n",
    "print(customers_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in customers_df:\")\n",
    "print(customers_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc92ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicated values\n",
    "print(f\"Unique customer: {customers_df.nunique()}\")\n",
    "print(f\"Duplicated values in customer_df: {customers_df.duplicated().sum()}\")\n",
    "print(f\"Duplicated values in customer_df['customer_id']: {customers_df['customer_id'].duplicated().sum()}\")\n",
    "# Expected as there are repeat buyers\n",
    "print(f\"Duplicated values in customer_df['customer_unique_id']: {customers_df['customer_unique_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524e914",
   "metadata": {},
   "source": [
    "Here we can see out of 99441 only 3345 has purchase more than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a33d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize city and state columns\n",
    "print(f\"Unique cities before standardization: {customers_df['customer_city'].nunique()}\")\n",
    "customers_df['customer_city'] = customers_df['customer_city'].str.strip().str.lower()\n",
    "customers_df['customer_state'] = customers_df['customer_state'].str.strip().str.lower()\n",
    "print(f\"Unique cities after standardization: {customers_df['customer_city'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98559193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in customers_df\n",
    "numerical_cols_customers = customers_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in customers_df:\", numerical_cols_customers)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_customers:\n",
    "    detect_and_plot_outliers_iqr(customers_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.to_parquet(\"olist_customers_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303c2fe",
   "metadata": {},
   "source": [
    "=== Olist Customers Dataset Pre-processing Summary ===\n",
    "\n",
    "1. DATA QUALITY & INTEGRITY:\n",
    "   - Missing Values: Confirmed ZERO missing values across all columns. The data foundation is 100% complete.\n",
    "   - Duplicate Rows: ZERO duplicate rows in the DataFrame.\n",
    "   - Key Duplicates: Confirmed no duplicates in the transactional key (`customer_id`), ensuring a unique link to the orders table.\n",
    "   - Unique ID Logic: Duplicates in `customer_unique_id` are *expected* and represent repeat buyers.\n",
    "\n",
    "2. DATA TYPE & STANDARDIZATION:\n",
    "   - Data Types: All columns were correctly assigned appropriate data types (no date/time conversions needed).\n",
    "   - Geographic Standardization: Successfully standardized `customer_city` and `customer_state` (trimmed and converted to lowercase).\n",
    "   - Rationale: Ensures consistency for future geographical feature engineering and embedding techniques.\n",
    "\n",
    "3. OUTLIER ANALYSIS:\n",
    "   - Outliers: No outliers were detected in the numerical column (`customer_zip_code_prefix`) using the Interquartile Range (IQR) method.\n",
    "   - Rationale: This confirmed the geographical prefix data is well-behaved and reflective of real-world distribution.\n",
    "\n",
    "4. ML READINESS:\n",
    "   - Features are clean and ready for joining. The core distinction between `customer_id` (transactional link) and `customer_unique_id` (individual tracking) is preserved for advanced modeling.\n",
    "\n",
    "Dataset saved to: 'olist_customers_cleaned_dataset.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40258db",
   "metadata": {},
   "source": [
    "### Processing the `olist_sellers_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07916645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sellers dataset\n",
    "seller_dataset_path = os.path.join(path, \"olist_sellers_dataset.csv\")\n",
    "seller_df = pd.read_csv(seller_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "seller_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a61b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the seller_df DataFrame\n",
    "print(seller_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in seller_df:\")\n",
    "print(seller_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf873820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicated values\n",
    "print(f\"Duplicated values in seller_df: {seller_df.duplicated().sum()}\")\n",
    "print(f\"Duplicated values in sellerer_df['seller_id']: {seller_df['seller_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38291a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize city and state columns\n",
    "print(f\"Unique seller cities before standardization: {seller_df['seller_city'].nunique()}\")\n",
    "seller_df['seller_city'] = seller_df['seller_city'].str.strip().str.lower()\n",
    "seller_df['seller_state'] = seller_df['seller_state'].str.strip().str.lower()\n",
    "print(f\"Unique seller cities after standardization: {seller_df['seller_city'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in seller_df\n",
    "numerical_cols_sellers = seller_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in seller_df:\", numerical_cols_sellers)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_sellers:\n",
    "    detect_and_plot_outliers_iqr(seller_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87911bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_df.to_parquet(\"olist_sellers_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b416e3",
   "metadata": {},
   "source": [
    "=== Olist Sellers Dataset Pre-processing Summary ===\n",
    "\n",
    "1. DATA QUALITY & INTEGRITY:\n",
    "   - Missing Values: Confirmed ZERO missing values across all columns. The seller data is perfectly complete.\n",
    "   - Duplicate Rows: ZERO duplicate rows in the DataFrame, ensuring each entry represents a unique seller.\n",
    "   - Key Integrity: Confirmed no duplicates in `seller_id`, which is the unique identifier and transactional link for this table.\n",
    "\n",
    "2. DATA TYPE & STANDARDIZATION:\n",
    "   - Data Types: All columns were correctly assigned appropriate data types.\n",
    "   - Geographic Standardization: Successfully standardized `seller_city` and `seller_state` (trimmed and converted to lowercase).\n",
    "   - Rationale: Prepares geographic columns for robust feature encoding and supports regional logistics analysis.\n",
    "\n",
    "3. OUTLIER ANALYSIS:\n",
    "   - Outliers: No outliers were detected in the numerical column (`seller_zip_code_prefix`) using the Interquartile Range (IQR) method.\n",
    "   - Rationale: Confirmed the geographical integrity of the seller distribution data.\n",
    "\n",
    "4. ML READINESS:\n",
    "   - Features are clean, complete, and standardized. The table is ready to be joined with order items to calculate critical logistics features like Seller-to-Customer Distance.\n",
    "\n",
    "Dataset saved to: 'olist_sellers_cleaned_dataset.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c2fb6",
   "metadata": {},
   "source": [
    "### Processing the `olist_order_reviews_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0981b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the order reviews dataset\n",
    "order_reviews_dataset_path = os.path.join(path, \"olist_order_reviews_dataset.csv\")\n",
    "order_reviews_df = pd.read_csv(order_reviews_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "order_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the order_reviews_df DataFrame\n",
    "print(order_reviews_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in order_reviews_df:\")\n",
    "print(order_reviews_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'review_comment_title' and 'review_comment_message' with 'no comment'\n",
    "order_reviews_df['review_comment_title'] = order_reviews_df['review_comment_title'].fillna('no comment title')\n",
    "order_reviews_df['review_comment_message'] = order_reviews_df['review_comment_message'].fillna('no comment')\n",
    "\n",
    "# Verify that missing values have been handled\n",
    "print(\"\\nMissing values in order_reviews_df after filling:\")\n",
    "print(order_reviews_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype mismatch\n",
    "order_reviews_df['review_answer_timestamp'] = pd.to_datetime(order_reviews_df['review_answer_timestamp'])\n",
    "order_reviews_df['review_creation_date'] = pd.to_datetime(order_reviews_df['review_creation_date'])\n",
    "\n",
    "print(f\"Datatype after being handled carefully: \\n{order_reviews_df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated Values\n",
    "print(f\"Duplicated values in order_reviews_df: {order_reviews_df.duplicated().sum()}\")\n",
    "print(f\"Duplicated values in order_reviews_df['review_id']: {order_reviews_df['review_id'].duplicated().sum()}\")\n",
    "print(f\"Duplicated values percentage in order_reviews_df['review_id']: {order_reviews_df['review_id'].duplicated().sum() / len(order_reviews_df)}\")\n",
    "\n",
    "# Remove duplicate review_id values from order_reviews_df\n",
    "order_reviews_df.drop_duplicates(subset='review_id', inplace=True)\n",
    "\n",
    "# Verify that duplicates have been removed\n",
    "print(f\"Duplicated values in order_reviews_df['review_id'] after removing duplicates: {order_reviews_df['review_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in seller_df\n",
    "numerical_cols_sellers = order_reviews_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in seller_df:\", numerical_cols_sellers)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_sellers:\n",
    "    detect_and_plot_outliers_iqr(order_reviews_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7126098",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_reviews_df.to_parquet(\"olist_order_reviews_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccac32b",
   "metadata": {},
   "source": [
    "=== Olist Order Reviews Dataset Pre-processing Summary ===\n",
    "\n",
    "1. DATA QUALITY & INTEGRITY:\n",
    "   - Missing Values: Addressed substantial nulls in `review_comment_title` and `review_comment_message` (approx. 88% missing).\n",
    "   - Imputation Strategy: Filled all missing textual fields with the marker **'no comment'**.\n",
    "   - Rationale: A missing comment is a *meaningful category* for sentiment analysis, distinguishing passive scores from active feedback.\n",
    "   - Duplicate Removal: Identified and removed **814 duplicated `review_id` values** to ensure data integrity and a clean 1:1 relationship with orders.\n",
    "\n",
    "2. DATA TYPE & TEMPORAL VALIDATION:\n",
    "   - Data Type Conversion: Converted `review_creation_date` and `review_answer_timestamp` to **datetime** objects.\n",
    "   - ML Rationale: Enables calculation of **Review Response Time** (answer - creation), a critical feature for platform/seller engagement metrics.\n",
    "\n",
    "3. OUTLIER ANALYSIS (The Score Problem):\n",
    "   - Outliers Detected: The IQR method flagged a large number of low scores (1s and 2s) as statistical outliers (approx. 14,396 rows).\n",
    "   - Decision: **ROWS WERE RETAINED.**\n",
    "   - Rationale: These are *not* errors; they are **high-leverage, negative feedback** essential for training any Customer Satisfaction or Churn model. Their removal would bias the model toward positive outcomes.\n",
    "\n",
    "4. ML READINESS:\n",
    "   - The table is the primary source for the target variable (`review_score`) and contains key features for NLP (via the comment fields) and temporal analysis.\n",
    "\n",
    "Dataset saved to: 'olist_order_reviews_cleaned_dataset.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51540028",
   "metadata": {},
   "source": [
    "### Processing the `olist_order_items_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the order items dataset\n",
    "order_items_dataset_path = os.path.join(path, \"olist_order_items_dataset.csv\")\n",
    "order_items_df = pd.read_csv(order_items_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "order_items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the order_items_df DataFrame\n",
    "print(order_items_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in order_items_df:\")\n",
    "print(order_items_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2901909",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_df.rename(columns={\"shipping_limit_date\" : \"shipping_deadline\"}, inplace=True)\n",
    "order_items_df['shipping_deadline'] = pd.to_datetime(order_items_df['shipping_deadline'])\n",
    "print(f\"Datatype after being handled carefully: \\n{order_items_df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated Values\n",
    "print(f\"Duplicated values in order_reviews_df: {order_items_df.duplicated().sum()}\")\n",
    "print(f\"Checking duplicates in the combination of order_id, item_is: {order_items_df.duplicated(subset=['order_id', 'order_item_id']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f11dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in order_items_df\n",
    "numerical_cols_items = order_items_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in order_items_df:\", numerical_cols_items)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_items:\n",
    "    if col == 'order_item_id': # As order item id is not an outliers\n",
    "      continue\n",
    "    detect_and_plot_outliers_iqr(order_items_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b479960",
   "metadata": {},
   "source": [
    "To ensure that the outliers present are actually really high price for products and high delivery charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab68d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the outliers without removing them first\n",
    "# Select only numerical columns for quantile calculation\n",
    "numerical_order_items_df = order_items_df.select_dtypes(include=np.number)\n",
    "\n",
    "Q1 = numerical_order_items_df.quantile(0.25)\n",
    "Q3 = numerical_order_items_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers for 'price' and 'freight_value' using their specific bounds\n",
    "outliers_price = order_items_df[(order_items_df['price'] < lower_bound['price']) | (order_items_df['price'] > upper_bound['price'])]\n",
    "outliers_freight = order_items_df[(order_items_df['freight_value'] < lower_bound['freight_value']) | (order_items_df['freight_value'] > upper_bound['freight_value'])]\n",
    "\n",
    "print(\"--- Top 5 Price Outliers ---\")\n",
    "display(outliers_price.sort_values('price', ascending=False).head())\n",
    "\n",
    "print(\"\\n--- Top 5 Freight Value Outliers ---\")\n",
    "display(outliers_freight.sort_values('freight_value', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599aaaf",
   "metadata": {},
   "source": [
    "Performing log transformation to ensure that we don't lose this cruicial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ddada",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_df['price_log'] = np.log1p(order_items_df['price'])\n",
    "order_items_df['freight_value_log'] = np.log1p(order_items_df['freight_value'])\n",
    "\n",
    "print(\"Log-transformed columns 'price_log' and 'freight_value_log' have been created.\")\n",
    "\n",
    "\n",
    "# --- Step 2: Visualize the \"Before and After\" ---\n",
    "# This will clearly show you why this method is so effective.\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Original distributions\n",
    "sns.histplot(order_items_df['price'], bins=50, kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Original Price Distribution (Skewed)')\n",
    "\n",
    "sns.histplot(order_items_df['freight_value'], bins=50, kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Original Freight Value Distribution (Skewed)')\n",
    "\n",
    "# Log-transformed distributions\n",
    "sns.histplot(order_items_df['price_log'], bins=50, kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Log-Transformed Price Distribution (Normalized)')\n",
    "\n",
    "sns.histplot(order_items_df['freight_value_log'], bins=50, kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Log-Transformed Freight Value Distribution (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_df.to_parquet(\"olist_order_items_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca506bc",
   "metadata": {},
   "source": [
    "=== Olist Order Items Dataset Pre-processing Summary ===\n",
    "\n",
    "1. DATA QUALITY & INTEGRITY:\n",
    "   - Missing Values: Confirmed ZERO missing values across the dataset. Data is 100% complete.\n",
    "   - Duplicate Rows: Confirmed ZERO duplicate rows. Uniqueness is ensured by the composite key (`order_id`, `order_item_id`).\n",
    "\n",
    "2. DATA TYPE & TEMPORAL VALIDATION:\n",
    "   - Data Type Conversion: Converted `shipping_limit_date` to a **datetime** object (renamed to `shipping_deadline` for clarity).\n",
    "   - ML Rationale: This date is crucial for calculating seller performance metrics (adherence to shipping timelines).\n",
    "\n",
    "3. OUTLIER MANAGEMENT (The Skewness Problem):\n",
    "   - Outliers Detected: Significant right-skewed outliers were detected in both `price` and `freight_value`. These represent real, high-value transactions (e.g., luxury items or large shipments).\n",
    "   - Decision: **ROWS WERE RETAINED.** Deletion would lead to severe data loss and bias.\n",
    "   - Transformation Strategy: Applied the **Logarithmic Transformation** ($\\log(1+x)$) to create **`price_log`** and **`freight_value_log`**.\n",
    "   - Rationale: The log transform stabilizes variance, pulls in the extreme tails, and normalizes the distribution, making these features compatible with linear models and ensuring efficient training without losing the influence of high-value items. \n",
    "\n",
    "4. ML READINESS:\n",
    "   - The transformed features (`price_log`, `freight_value_log`) are now robust, stable inputs for **Revenue Forecasting** and **Logistics Cost Modeling**.\n",
    "\n",
    "Dataset saved to: 'olist_order_items_cleaned_dataset.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba6966",
   "metadata": {},
   "source": [
    "### Processing the `olist_products_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cce8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the products dataset\n",
    "products_dataset_path = os.path.join(path, \"olist_products_dataset.csv\")\n",
    "products_df = pd.read_csv(products_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a05351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the products_df DataFrame\n",
    "print(products_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in products_df:\")\n",
    "print(products_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df.rename(columns={\n",
    "    \"product_name_lenght\" : \"product_name_length\",\n",
    "    \"product_description_lenght\" : \"product_description_length\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3decb061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'product_category_name' with 'unknown'\n",
    "products_df['product_category_name'] = products_df['product_category_name'].fillna('unknown')\n",
    "\n",
    "# Verify that missing values in 'product_category_name' have been handled\n",
    "print(\"\\nMissing values in products_df after filling 'product_category_name':\")\n",
    "print(products_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median of 'product_name_length', 'product_description_length', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', and 'product_width_cm'\n",
    "median_name_length = products_df['product_name_length'].median()\n",
    "median_description_length = products_df['product_description_length'].median()\n",
    "median_photos_qty = products_df['product_photos_qty'].median()\n",
    "median_weight_g = products_df['product_weight_g'].median()\n",
    "median_length_cm = products_df['product_length_cm'].median()\n",
    "median_height_cm = products_df['product_height_cm'].median()\n",
    "median_width_cm = products_df['product_width_cm'].median()\n",
    "\n",
    "\n",
    "# Impute missing values in 'product_name_length', 'product_description_length', and 'product_photos_qty' with their medians\n",
    "products_df['product_name_length'] = products_df['product_name_length'].fillna(median_name_length)\n",
    "products_df['product_description_length'] = products_df['product_description_length'].fillna(median_description_length)\n",
    "products_df['product_photos_qty'] = products_df['product_photos_qty'].fillna(median_photos_qty)\n",
    "\n",
    "# Impute missing values in 'product_weight_g', 'product_length_cm', 'product_height_cm', and 'product_width_cm' with their medians\n",
    "products_df['product_weight_g'] = products_df['product_weight_g'].fillna(median_weight_g)\n",
    "products_df['product_length_cm'] = products_df['product_length_cm'].fillna(median_length_cm)\n",
    "products_df['product_height_cm'] = products_df['product_height_cm'].fillna(median_height_cm)\n",
    "products_df['product_width_cm'] = products_df['product_width_cm'].fillna(median_width_cm)\n",
    "\n",
    "\n",
    "# Verify that missing values in 'product_name_length' have been handled\n",
    "print(\"\\nMissing values in products_df after imputing numerical columns:\")\n",
    "print(products_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated Values\n",
    "print(f\"Duplicated values in order_reviews_df: {products_df.duplicated().sum()}\")\n",
    "print(f\"Duplicated values in order_reviews_df['product_id']: {products_df['product_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a532845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in order_items_df\n",
    "numerical_cols_items = products_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in order_items_df:\", numerical_cols_items)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_items:\n",
    "    detect_and_plot_outliers_iqr(products_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9175f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df['product_weight_g_log'] = np.log1p(products_df['product_weight_g'])\n",
    "products_df['product_volume_cm3'] = products_df['product_length_cm'] * products_df['product_height_cm'] * products_df['product_width_cm']\n",
    "products_df['product_volume_cm3_log'] = np.log1p(products_df['product_volume_cm3'])\n",
    "\n",
    "print(\"Log-transformed columns for weight and volume have been created.\")\n",
    "\n",
    "# --- 2. Cap the Product Description Length ---\n",
    "# Calculate the 99th percentile\n",
    "desc_len_cap = products_df['product_description_length'].quantile(0.99)\n",
    "print(f\"Product description length will be capped at: {desc_len_cap:.0f} characters.\")\n",
    "\n",
    "# Create a new capped column\n",
    "products_df['product_description_length_capped'] = products_df['product_description_length'].clip(upper=desc_len_cap)\n",
    "\n",
    "print(\"Capped column for description length has been created.\")\n",
    "\n",
    "# --- 3. Do Nothing for Photos Qty and Name Length ---\n",
    "print(\"No changes made to 'product_photos_qty' or 'product_name_length'.\")\n",
    "\n",
    "# Display the new columns\n",
    "print(\"\\nDataFrame with new transformed/capped columns:\")\n",
    "display(products_df[['product_weight_g', 'product_weight_g_log', 'product_volume_cm3', 'product_volume_cm3_log', 'product_description_length', 'product_description_length_capped']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc69be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df.to_parquet(\"olist_products_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916e98c",
   "metadata": {},
   "source": [
    "=== Olist Products Dataset Pre-processing Summary ===\n",
    "\n",
    "1. DATA QUALITY & INTEGRITY:\n",
    "   - Missing Values: Addressed non-random nulls across multiple columns.\n",
    "   - Categorical Imputation: Filled ~1.85% missing `product_category_name` values with the string **'unknown'**.\n",
    "   - Numerical Imputation: Filled remaining missing values in physical attributes (length, height, width, weight, name/description lengths, photo quantity) using the **median**.\n",
    "   - Rationale: The median is a robust statistic, preserving the central tendency and avoiding skew when imputing product dimensions for logistics features.\n",
    "   - Duplicate Rows: Confirmed ZERO duplicate rows (each row is a unique `product_id`).\n",
    "\n",
    "2. FEATURE ENGINEERING (Dimensionality):\n",
    "   - Feature Creation: Explicitly created a new, crucial feature: **`product_volume_cm3`** (Length * Height * Width). This is a strong, derived predictor for freight cost.\n",
    "   - Log Transformation (Weight): Applied **Logarithmic Transformation** ($\\log(1+x)$) to `product_weight_g` to create **`product_weight_g_log`**.\n",
    "   - Log Transformation (Volume): Applied **Logarithmic Transformation** ($\\log(1+x)$) to the newly created `product_volume_cm3` to create **`product_volume_cm3_log`**.\n",
    "   - Rationale: This transformation minimizes the influence of large, sparse outliers while keeping the data distribution stable for linear algorithms.\n",
    "\n",
    "3. OUTLIER MANAGEMENT (Capping):\n",
    "   - Targeted Capping: The `product_description_length` was managed using **Percentile Capping** at the **99th percentile**.\n",
    "   - Rationale: Capping (Winsorizing) limits the influence of the few extremely verbose product descriptions without removing real data points, maintaining a stable numerical representation of text complexity.\n",
    "\n",
    "4. ML READINESS:\n",
    "   - The table provides high-quality, dense features for **Product Recommendation Systems** (using categories and descriptions) and **Logistics/Pricing Models** (using transformed physical attributes).\n",
    "\n",
    "Dataset saved to: 'olist_products_cleaned_dataset.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dbfb7f",
   "metadata": {},
   "source": [
    "### Processing the `olist_geolocation_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geolocation dataset\n",
    "geolocation_dataset_path = os.path.join(path, \"olist_geolocation_dataset.csv\")\n",
    "geolocation_df = pd.read_csv(geolocation_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "geolocation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the geolocation_df DataFrame\n",
    "geolocation_df_info = geolocation_df.info()\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in geolocation_df:\")\n",
    "print(geolocation_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49badbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows in the geolocation_df\n",
    "print(f\"Number of duplicated rows in geolocation_df: {geolocation_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27453b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows based on the subset of specified columns\n",
    "geolocation_df.drop_duplicates(subset=['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng'], keep='first', inplace=True)\n",
    "\n",
    "# Verify that duplicates based on the subset have been removed\n",
    "print(f\"Number of duplicated rows based on zip code, lat, and lng after removing duplicates: {geolocation_df.duplicated(subset=['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in order_items_df\n",
    "numerical_cols_items = geolocation_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in order_items_df:\", numerical_cols_items)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_items:\n",
    "    detect_and_plot_outliers_iqr(geolocation_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b983928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the approximate geographical boundaries for Brazil\n",
    "LAT_MIN, LAT_MAX = -34, 6\n",
    "LON_MIN, LON_MAX = -74, -34\n",
    "\n",
    "# Find coordinates that fall outside these boundaries\n",
    "geo_outliers = geolocation_df[\n",
    "    (geolocation_df['geolocation_lat'] < LAT_MIN) | (geolocation_df['geolocation_lat'] > LAT_MAX) |\n",
    "    (geolocation_df['geolocation_lng'] < LON_MIN) | (geolocation_df['geolocation_lng'] > LON_MAX)\n",
    "]\n",
    "\n",
    "num_outliers = len(geo_outliers)\n",
    "\n",
    "if num_outliers > 0:\n",
    "    print(f\"--- Geographic Outlier Analysis ---\")\n",
    "    print(f\"Found {num_outliers} coordinates outside the plausible boundaries of Brazil.\")\n",
    "    print(\"\\nDisplaying some of the detected outliers:\")\n",
    "    display(geo_outliers)\n",
    "\n",
    "    geolocation_df_cleaned = geolocation_df.drop(geo_outliers.index)\n",
    "    print(f\"\\n{num_outliers} outlier rows have been removed.\")\n",
    "\n",
    "else:\n",
    "    print(\"No geographic outliers found outside the plausible boundaries of Brazil.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation_df.to_parquet(\"olist_geolocation_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f5382",
   "metadata": {},
   "source": [
    "=== Olist Geolocation Dataset Pre-processing Summary ===\n",
    "\n",
    "1. DATA QUALITY & INTEGRITY (Data Reduction):\n",
    "   - Missing Values: Confirmed ZERO missing values.\n",
    "   - Duplicate Reduction: The raw file contained over 1 million rows, many of which were redundant mappings.\n",
    "   - Strategy: Dropped duplicates based on the key combination (`geolocation_zip_code_prefix`, `geolocation_lat`, `geolocation_lng`).\n",
    "\n",
    "2. OUTLIER MANAGEMENT (Geographic Integrity):\n",
    "   - Outlier Detection: Implemented a rigorous **Geographic Boundary Check** on latitude and longitude columns.\n",
    "   - Strategy: Removed all coordinates falling outside the plausible geographical boundaries of Brazil (LAT $\\in [-34, 6]$, LON $\\in [-74, -34]$).\n",
    "   - Rationale: Any coordinates outside these bounds represent non-physical data entry errors. This step ensures that our distance calculations are based on accurate, valid locations, eliminating noise from logistical models.\n",
    "\n",
    "3. ML READINESS:\n",
    "   - Distance Matrix Source: The cleaned file is now a high-fidelity lookup table ready to be joined with the Customers and Sellers tables.\n",
    "   - Primary Feature Input: This enables the generation of the single most crucial feature for logistics prediction: **Seller-to-Customer Great-Circle Distance**, directly impacting freight cost and delivery time models.\n",
    "\n",
    "Dataset saved to: 'olist_geolocation_cleaned_dataset.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260b463",
   "metadata": {},
   "source": [
    "### Processing the `product_category_name_translation_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the product category name translation dataset\n",
    "category_name_translation_dataset_path = os.path.join(path, \"product_category_name_translation.csv\")\n",
    "category_name_translation_df = pd.read_csv(category_name_translation_dataset_path)\n",
    "\n",
    "# Display the entire DataFrame\n",
    "category_name_translation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1412f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the category_name_translation_df DataFrame\n",
    "print(category_name_translation_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in category_name_translation_df:\")\n",
    "print(category_name_translation_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicated values in category_name_translation_df: {category_name_translation_df.duplicated().sum()}\")\n",
    "print(f\"Duplicated values in category_name_translation_df['product_category_name']: {category_name_translation_df['product_category_name'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f896194",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name_translation_df.to_parquet(\"category_name_translation_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185333d4",
   "metadata": {},
   "source": [
    "=== Product Category Translation Dataset Pre-processing Summary ===\n",
    "\n",
    "1. DATA QUALITY & INTEGRITY:\n",
    "   - Missing Values: Confirmed ZERO missing values.\n",
    "   - Duplicate Rows: Confirmed ZERO duplicate rows.\n",
    "   - Rationale: The data quality is pristine; the file functions as a high-integrity, static lookup table.\n",
    "\n",
    "2. FEATURE ENGINEERING MANDATE (Integration):\n",
    "   - Strategy: This file serves a singular, non-negotiable role: **to enable human and model interpretability**.\n",
    "   - Action: No transformation or cleaning was applied as the data is already standardized. The file is ready to be merged directly with the `olist_products_cleaned_dataset.parquet`.\n",
    "\n",
    "3. ML READINESS:\n",
    "   - Interpretability Feature: The join allows us to replace the original Portuguese category names (high cognitive load) with their English translations, creating the feature **`product_category_name_english`**.\n",
    "   - Rationale: This translation is fundamental for clear **Exploratory Data Analysis (EDA)**, simplified **Feature Importance** analysis (e.g., in tree-based models), and universal comprehension of our **Product Segmentation** models.\n",
    "\n",
    "Dataset saved to: 'category_name_translation_cleaned_dataset.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cdfb5a",
   "metadata": {},
   "source": [
    "### Processing the `olist_orders_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the orders dataset\n",
    "orders_dataset_path = os.path.join(path, \"olist_orders_dataset.csv\")\n",
    "orders_df = pd.read_csv(orders_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8590d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the orders_df DataFrame\n",
    "print(orders_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in orders_df:\")\n",
    "print(orders_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb644d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\",\n",
    "    \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\"\n",
    "]\n",
    "\n",
    "for col in date_cols:\n",
    "    orders_df[col] = pd.to_datetime(orders_df[col])\n",
    "\n",
    "orders_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where 'order_status' is 'delivered' and overwrite the original DataFrame\n",
    "orders_df = orders_df[orders_df['order_status'] == 'delivered']\n",
    "\n",
    "# Now, drop rows with missing delivery dates directly from 'orders_df'\n",
    "orders_df.dropna(subset=['order_delivered_customer_date'], inplace=True)\n",
    "\n",
    "print(\"Original DataFrame has been modified. \")\n",
    "print(\"\\nShape after filtering and dropping nulls:\", orders_df.shape)\n",
    "print(\"\\nRemaining missing values:\")\n",
    "print(orders_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97443e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.dropna(subset=['order_approved_at', 'order_delivered_carrier_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0334e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "approved_before_purchase = orders_df['order_approved_at'] < orders_df['order_purchase_timestamp']\n",
    "carrier_before_approved = orders_df['order_delivered_carrier_date'] < orders_df['order_approved_at']\n",
    "customer_before_carrier = orders_df['order_delivered_customer_date'] < orders_df['order_delivered_carrier_date']\n",
    "estimated_before_purchase = orders_df['order_estimated_delivery_date'] < orders_df['order_purchase_timestamp']\n",
    "\n",
    "invalid_order_conditions = (\n",
    "    approved_before_purchase |\n",
    "    carrier_before_approved |\n",
    "    customer_before_carrier |\n",
    "    estimated_before_purchase\n",
    ")\n",
    "\n",
    "invalid_orders = orders_df[invalid_order_conditions].copy()\n",
    "\n",
    "print(\"--- Inspection of Invalid Orders (Temporal Causality Violations) ---\")\n",
    "print(f\"Total number of orders identified as invalid: {len(invalid_orders)}\")\n",
    "\n",
    "if not invalid_orders.empty:\n",
    "    print(\"\\nSample of Invalid Orders (Showing Order ID and Timestamps):\\n\")\n",
    "    display_cols = [\n",
    "        'order_id', \n",
    "        'order_purchase_timestamp', \n",
    "        'order_approved_at', \n",
    "        'order_delivered_carrier_date', \n",
    "        'order_delivered_customer_date', \n",
    "        'order_estimated_delivery_date'\n",
    "    ]\n",
    "    \n",
    "    print(invalid_orders[display_cols].head())\n",
    "\n",
    "    print(\"\\nReasons for the first 5 Invalid Orders (True = Violation):\\n\")\n",
    "    reasons = pd.DataFrame({\n",
    "        'Approved < Purchase': approved_before_purchase.loc[invalid_orders.index].head(),\n",
    "        'Carrier < Approved': carrier_before_approved.loc[invalid_orders.index].head(),\n",
    "        'Customer < Carrier': customer_before_carrier.loc[invalid_orders.index].head(),\n",
    "        'Estimated < Purchase': estimated_before_purchase.loc[invalid_orders.index].head(),\n",
    "    })\n",
    "    print(reasons)\n",
    "\n",
    "orders_df_cleaned = orders_df[~invalid_order_conditions].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nNumber of orders remaining after removal: {len(orders_df_cleaned)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ade39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicated values in category_name_translation_df: {orders_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc832d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.to_parquet(\"olist_orders_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbf576",
   "metadata": {},
   "source": [
    "--- ORDERS DATASET PRE-PROCESSING SUMMARY (olist_orders_dataset.csv) ---\n",
    "\n",
    "1. Data Filtering (Filtering for Success):\n",
    "    - ACTION: Filtered dataset to retain only orders with 'order_status' == 'delivered'.\n",
    "    - ML RATIONALE: This creates the essential 'Ground Truth' for supervised learning models focused on successful delivery, customer satisfaction, and logistics. Incomplete/cancelled orders are noise for these tasks.\n",
    "\n",
    "2. Temporal Integrity & Type Conversion:\n",
    "    - ACTION: Converted all 5 date/timestamp columns (e.g., 'order_purchase_timestamp', 'order_delivered_customer_date') to datetime objects.\n",
    "    - ML RATIONALE: Correct typing is non-negotiable for deriving features like lead times, delivery gaps, and time-series analysis.\n",
    "\n",
    "3. Null Value Strategy (Targeted Removal):\n",
    "    - ACTION: Rows with missing final delivery dates ('order_delivered_customer_date') were dropped, as they are unresolvable data quality issues for a 'delivered' order.\n",
    "    - ACTION: Nulls in critical internal timestamps ('order_approved_at', 'order_delivered_carrier_date') were also dropped to ensure temporal calculations are clean.\n",
    "\n",
    "4. Chronological Validation (Causality Check):\n",
    "    - ACTION: Explicitly removed orders violating logical time sequence (e.g., approval before purchase, delivery before carrier handoff).\n",
    "    - ML RATIONALE: This step eliminates orders with impossible, non-physical time differences, protecting the integrity of all subsequently calculated time-based features from corruption.\n",
    "\n",
    "5. Outliers & Duplicates:\n",
    "    - ACTION: Confirmed zero duplicate rows and no standard numerical outliers (as this dataset is primarily composed of categorical and temporal data).\n",
    "\n",
    "ML READINESS: This table now forms the robust central hub, temporally consistent and filtered to the target population of successful transactions, ready for high-fidelity feature joining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed626b0",
   "metadata": {},
   "source": [
    "### Processing the `olist_order_payments_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the order payments dataset\n",
    "order_payments_dataset_path = os.path.join(path, \"olist_order_payments_dataset.csv\")\n",
    "order_payments_df = pd.read_csv(order_payments_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "order_payments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the order_payments_df DataFrame\n",
    "order_payments_df_info = order_payments_df.info()\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in order_payments_df:\")\n",
    "print(order_payments_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe34028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in order_items_df\n",
    "numerical_cols_items = order_payments_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in order_items_df:\", numerical_cols_items)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_items:\n",
    "    detect_and_plot_outliers_iqr(order_payments_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets to get full order context\n",
    "payments_and_orders_df = pd.merge(order_payments_df, orders_df, on='order_id')\n",
    "full_order_details_df = pd.merge(payments_and_orders_df, order_items_df, on='order_id')\n",
    "\n",
    "# Sort by payment_value to see the largest transactions\n",
    "top_payments = full_order_details_df.sort_values(by='payment_value', ascending=False)\n",
    "\n",
    "# Display the top 10 largest payments and their details\n",
    "print(\"\\nTop 10 Largest Transactions:\")\n",
    "print(top_payments.head(10)[['order_id', 'payment_value', 'price', 'freight_value', 'product_id']])\n",
    "\n",
    "# Investigate zero-value payments\n",
    "zero_value_payments = order_payments_df[order_payments_df['payment_value'] == 0]\n",
    "print(f\"\\nNumber of zero-value payments: {len(zero_value_payments)}\")\n",
    "print(\"Payment types for zero-value transactions:\")\n",
    "print(zero_value_payments['payment_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89be65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the 99.9th percentile\n",
    "cap_value = order_payments_df['payment_value'].quantile(0.999)\n",
    "print(f\"\\n99.9th percentile value (capping threshold): {cap_value:.2f}\")\n",
    "\n",
    "# Create a new column with the capped values\n",
    "order_payments_df['capped_payment_value'] = order_payments_df['payment_value'].clip(upper=cap_value)\n",
    "\n",
    "# Compare the original and capped statistics\n",
    "print(\"\\nStatistics after capping:\")\n",
    "print(order_payments_df[['payment_value', 'capped_payment_value']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_payments_df.to_parquet(\"olist_order_payments_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace60444",
   "metadata": {},
   "source": [
    "--- PAYMENTS DATASET PRE-PROCESSING SUMMARY (olist_order_payments_dataset.csv) ---\n",
    "\n",
    "1. Data Quality Check:\n",
    "    - ACTION: Confirmed no missing values and zero duplicate rows in the raw dataset.\n",
    "    - ML RATIONALE: The transactional ledger has high initial integrity, ensuring clean join keys (order_id) for aggregation.\n",
    "\n",
    "2. Outlier Management (The High-Value Challenge):\n",
    "    - ACTION: Applied **99.9th percentile capping (Winsorizing)** to the highly-skewed 'payment_value' to create a new feature: 'capped_payment_value'.\n",
    "    - ML RATIONALE: Financial data often contains extreme, high-leverage outliers (whale transactions). Capping them limits their disproportionate influence on model loss functions (especially L2/MSE loss), stabilizing the training process without removing the data points entirely.\n",
    "\n",
    "3. Zero-Value Validation:\n",
    "    - ACTION: Investigated payments with a value of $0.00. These were confirmed to primarily correspond to the **'voucher'** payment type.\n",
    "    - ML RATIONALE: Zero is a meaningful, descriptive value for vouchers (indicating full discount). These rows were retained as a distinct, predictive class, not treated as errors or missing data.\n",
    "\n",
    "4. Feature Integrity:\n",
    "    - ACTION: Retained the categorical feature 'payment_type' and the numerical feature 'payment_installments' as primary predictors for future **Credit Risk Modeling** or **Payment Preference Clustering**.\n",
    "\n",
    "ML READINESS: The table is financially robust. The new 'capped_payment_value' is now ready for stable use in Revenue/Margin prediction, ensuring no single transaction destabilizes the model's learning curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
