{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92bbdc46",
   "metadata": {},
   "source": [
    "### Brazilian E-Commerce Public Dataset by Olist\n",
    "This is a Brazilian ecommerce public dataset of orders made at Olist Store. The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers. We also released a geolocation dataset that relates Brazilian zip codes to lat/lng coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e252f0",
   "metadata": {},
   "source": [
    "The code in the following cell downloads the dataset from Kagglehub and lists the files in the downloaded directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25f076",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.env (Python 3.13.5)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"d:/Data Science/CaseStudy ML/Olist-E-Commerce-Predictor-/.env/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Download the latest version of the dataset\n",
    "path = kagglehub.dataset_download(\"olistbr/brazilian-ecommerce\")\n",
    "\n",
    "# List the files in the downloaded directory\n",
    "file_list = os.listdir(path)\n",
    "for file_name in file_list:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_plot_outliers_iqr(df, column, exclude_zero=False):\n",
    "    \"\"\"\n",
    "    Detects outliers using the IQR method, prints outlier information, and plots a box plot.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): The name of the column to analyze for outliers.\n",
    "        exclude_zero (bool): Whether to exclude zero values from outlier calculation.\n",
    "                             Useful for columns where zero is a meaningful non-outlier value.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Outlier Analysis for column: {column} ---\")\n",
    "\n",
    "    # Create box plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(y=df[column])\n",
    "    plt.title(f'Box plot of {column}')\n",
    "    plt.ylabel(column)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate IQR and identify outliers\n",
    "    if exclude_zero:\n",
    "        # Consider only non-zero values for IQR calculation\n",
    "        data_for_iqr = df[df[column] != 0][column]\n",
    "    else:\n",
    "        data_for_iqr = df[column]\n",
    "\n",
    "    if data_for_iqr.empty:\n",
    "        print(f\"  No non-zero data in column '{column}' to calculate outliers.\")\n",
    "        return\n",
    "\n",
    "    Q1 = data_for_iqr.quantile(0.25)\n",
    "    Q3 = data_for_iqr.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify outliers in the original DataFrame\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "\n",
    "    num_outliers = len(outliers)\n",
    "    percentage_outliers = (num_outliers / len(df)) * 100\n",
    "\n",
    "    print(f\"  Number of outliers: {num_outliers}\")\n",
    "    print(f\"  Percentage of outliers: {percentage_outliers:.2f}%\")\n",
    "    print(f\"  Lower bound (IQR): {lower_bound:.2f}\")\n",
    "    print(f\"  Upper bound (IQR): {upper_bound:.2f}\")\n",
    "\n",
    "    if num_outliers > 0 and num_outliers < 20: # Display if not too many outliers\n",
    "        print(\"\\nSample Outlier Rows:\")\n",
    "        display(outliers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5157b26",
   "metadata": {},
   "source": [
    "### Processing the `olist_customers_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215fbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the customers dataset\n",
    "customers_dataset_path = os.path.join(path, \"olist_customers_dataset.csv\")\n",
    "customers_df = pd.read_csv(customers_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the customers_df DataFrame\n",
    "print(customers_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in customers_df:\")\n",
    "print(customers_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc92ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicated values\n",
    "print(f\"Duplicated values in customer_df: {customers_df.duplicated().sum()}\")\n",
    "print(f\"Duplicated values in customer_df['customer_id']: {customers_df['customer_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a33d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize city and state columns\n",
    "print(f\"Unique cities before standardization: {customers_df['customer_city'].nunique()}\")\n",
    "customers_df['customer_city'] = customers_df['customer_city'].str.strip().str.lower()\n",
    "customers_df['customer_state'] = customers_df['customer_state'].str.strip().str.lower()\n",
    "print(f\"Unique cities after standardization: {customers_df['customer_city'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98559193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in customers_df\n",
    "numerical_cols_customers = customers_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in customers_df:\", numerical_cols_customers)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_customers:\n",
    "    detect_and_plot_outliers_iqr(customers_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.to_parquet(\"olist_customers_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303c2fe",
   "metadata": {},
   "source": [
    "Here are the key observations from the pre-processing of the `customers_df`:\n",
    "\n",
    "*   **Missing Values:** No missing values were found in the dataset.\n",
    "*   **Data Types:** All columns have been assigned appropriate data types.\n",
    "*   **Duplicate Values:** There are no duplicate rows in the entire DataFrame or in the `customer_id` column. Duplicates in the `customer_unique_id` column are expected and represent repeat buyers, as explained in the dataset documentation.\n",
    "*   **Row Uniqueness:** Each row represents a unique customer entry in this specific dataset instance.\n",
    "*   **Key Identifiers:** `customer_id` serves as an anonymized link to orders, while `customer_unique_id` allows for tracking individual customers across multiple orders.\n",
    "*   **Geographical Information:** The dataset includes `customer_city`, `customer_state`, and `customer_zip_code_prefix` for geographical analysis.\n",
    "*   **Outliers:** No outliers were detected in the numerical column (`customer_zip_code_prefix`) using the IQR method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40258db",
   "metadata": {},
   "source": [
    "### Processing the `olist_sellers_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07916645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sellers dataset\n",
    "seller_dataset_path = os.path.join(path, \"olist_sellers_dataset.csv\")\n",
    "seller_df = pd.read_csv(seller_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "seller_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a61b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the seller_df DataFrame\n",
    "print(seller_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in seller_df:\")\n",
    "print(seller_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf873820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicated values\n",
    "print(f\"Duplicated values in seller_df: {seller_df.duplicated().sum()}\")\n",
    "print(f\"Duplicated values in sellerer_df['seller_id']: {seller_df['seller_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38291a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize city and state columns\n",
    "print(f\"Unique seller cities before standardization: {seller_df['seller_city'].nunique()}\")\n",
    "seller_df['seller_city'] = seller_df['seller_city'].str.strip().str.lower()\n",
    "seller_df['seller_state'] = seller_df['seller_state'].str.strip().str.lower()\n",
    "print(f\"Unique seller cities after standardization: {seller_df['seller_city'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in seller_df\n",
    "numerical_cols_sellers = seller_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in seller_df:\", numerical_cols_sellers)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_sellers:\n",
    "    detect_and_plot_outliers_iqr(seller_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87911bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_df.to_parquet(\"olist_sellers_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b416e3",
   "metadata": {},
   "source": [
    "Here are the key observations from the pre-processing of the `seller_df`:\n",
    "\n",
    "*   **Missing Values and Data Types:** No missing values were detected, and all columns have appropriate data types.\n",
    "*   **Duplicate Values:** There are no duplicate values present in the DataFrame, ensuring each entry is unique per seller.\n",
    "*   **Row Uniqueness:** Each row corresponds to a unique seller identified by `seller_id`.\n",
    "*   **Geographical Information:** The dataset includes `seller_zip_code_prefix`, `seller_city`, and `seller_state`, allowing for geographical analysis of seller distribution.\n",
    "*   **Geographical Distribution:** The `seller_state` column indicates the Brazilian state where each seller is located, which is important for analyzing logistics and regional presence.\n",
    "*   **Outliers:** No outliers were detected in the numerical column (`seller_zip_code_prefix`) using the IQR method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c2fb6",
   "metadata": {},
   "source": [
    "### Processing the `olist_order_reviews_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0981b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the order reviews dataset\n",
    "order_reviews_dataset_path = os.path.join(path, \"olist_order_reviews_dataset.csv\")\n",
    "order_reviews_df = pd.read_csv(order_reviews_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "order_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the order_reviews_df DataFrame\n",
    "print(order_reviews_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in order_reviews_df:\")\n",
    "print(order_reviews_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'review_comment_title' and 'review_comment_message' with 'no comment'\n",
    "order_reviews_df['review_comment_title'] = order_reviews_df['review_comment_title'].fillna('no comment')\n",
    "order_reviews_df['review_comment_message'] = order_reviews_df['review_comment_message'].fillna('no comment')\n",
    "\n",
    "# Verify that missing values have been handled\n",
    "print(\"\\nMissing values in order_reviews_df after filling:\")\n",
    "print(order_reviews_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype mismatch\n",
    "order_reviews_df['review_answer_timestamp'] = pd.to_datetime(order_reviews_df['review_answer_timestamp'])\n",
    "order_reviews_df['review_creation_date'] = pd.to_datetime(order_reviews_df['review_creation_date'])\n",
    "\n",
    "print(f\"Datatype after being handled carefully: \\n{order_reviews_df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated Values\n",
    "print(f\"Duplicated values in order_reviews_df: {order_reviews_df.duplicated().sum()}\")\n",
    "print(f\"Duplicated values in order_reviews_df['review_id']: {order_reviews_df['review_id'].duplicated().sum()}\")\n",
    "# Remove duplicate review_id values from order_reviews_df\n",
    "order_reviews_df.drop_duplicates(subset='review_id', inplace=True)\n",
    "\n",
    "# Verify that duplicates have been removed\n",
    "print(f\"Duplicated values in order_reviews_df['review_id'] after removing duplicates: {order_reviews_df['review_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in seller_df\n",
    "numerical_cols_sellers = order_reviews_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in seller_df:\", numerical_cols_sellers)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_sellers:\n",
    "    detect_and_plot_outliers_iqr(order_reviews_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7126098",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_reviews_df.to_parquet(\"olist_order_reviews_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccac32b",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Each row corresponds to exactly one review per order.\n",
    "* review_score ranges from 1 to 5, with most orders having a rating.\n",
    "* Many reviews lack textual comments (review_comment_title and review_comment_message have many nulls) which were later replaced by 'no comment' string.\n",
    "* Review timestamps (review_creation_date, review_answer_timestamp) are stored as objects and will need datetime conversion for time series analysis.\n",
    "* we found that there are 814 duplicated values in the review_id.\n",
    "* This dataset can provide insights on customer satisfaction and correlate ratings with delivery times, sellers, or product categories during EDA.\n",
    "* Number of outliers: 14396, but these outliers are part of rating which are given my customers and it can be highly negative or highly positive but these are essential for the ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51540028",
   "metadata": {},
   "source": [
    "### Processing the `olist_order_items_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the order items dataset\n",
    "order_items_dataset_path = os.path.join(path, \"olist_order_items_dataset.csv\")\n",
    "order_items_df = pd.read_csv(order_items_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "order_items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the order_items_df DataFrame\n",
    "print(order_items_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in order_items_df:\")\n",
    "print(order_items_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2901909",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_df.rename(columns={\"shipping_limit_date\" : \"shipping_deadline\"}, inplace=True)\n",
    "order_items_df['shipping_deadline'] = pd.to_datetime(order_items_df['shipping_deadline'])\n",
    "print(f\"Datatype after being handled carefully: \\n{order_items_df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated Values\n",
    "print(f\"Duplicated values in order_reviews_df: {order_items_df.duplicated().sum()}\")\n",
    "print(f\"Checking duplicates in the combination of order_id, item_is: {order_items_df.duplicated(subset=['order_id', 'order_item_id']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f11dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in order_items_df\n",
    "numerical_cols_items = order_items_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in order_items_df:\", numerical_cols_items)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_items:\n",
    "    if col == 'order_item_id': # Exclude order_item_id from outlier detection\n",
    "      continue\n",
    "    detect_and_plot_outliers_iqr(order_items_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab68d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the outliers without removing them first\n",
    "# Select only numerical columns for quantile calculation\n",
    "numerical_order_items_df = order_items_df.select_dtypes(include=np.number)\n",
    "\n",
    "Q1 = numerical_order_items_df.quantile(0.25)\n",
    "Q3 = numerical_order_items_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers for 'price' and 'freight_value' using their specific bounds\n",
    "outliers_price = order_items_df[(order_items_df['price'] < lower_bound['price']) | (order_items_df['price'] > upper_bound['price'])]\n",
    "outliers_freight = order_items_df[(order_items_df['freight_value'] < lower_bound['freight_value']) | (order_items_df['freight_value'] > upper_bound['freight_value'])]\n",
    "\n",
    "print(\"--- Top 5 Price Outliers ---\")\n",
    "display(outliers_price.sort_values('price', ascending=False).head())\n",
    "\n",
    "print(\"\\n--- Top 5 Freight Value Outliers ---\")\n",
    "display(outliers_freight.sort_values('freight_value', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ddada",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_df['price_log'] = np.log1p(order_items_df['price'])\n",
    "order_items_df['freight_value_log'] = np.log1p(order_items_df['freight_value'])\n",
    "\n",
    "print(\"Log-transformed columns 'price_log' and 'freight_value_log' have been created.\")\n",
    "\n",
    "\n",
    "# --- Step 2: Visualize the \"Before and After\" ---\n",
    "# This will clearly show you why this method is so effective.\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Original distributions\n",
    "sns.histplot(order_items_df['price'], bins=50, kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Original Price Distribution (Skewed)')\n",
    "\n",
    "sns.histplot(order_items_df['freight_value'], bins=50, kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Original Freight Value Distribution (Skewed)')\n",
    "\n",
    "# Log-transformed distributions\n",
    "sns.histplot(order_items_df['price_log'], bins=50, kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Log-Transformed Price Distribution (Normalized)')\n",
    "\n",
    "sns.histplot(order_items_df['freight_value_log'], bins=50, kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Log-Transformed Freight Value Distribution (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_df.to_parquet(\"olist_order_items_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca506bc",
   "metadata": {},
   "source": [
    "Observations\n",
    "* No missing values detected in this dataset.\n",
    "* Each row represents an individual product item in an order:\n",
    "* An order can have multiple rows (one per product).\n",
    "* order_item_id is not a unique identifier by itself, but it helps identify the position of an item within a given order_id.\n",
    "* price and freight_value will be critical for revenue and cost analysis in the EDA stage.\n",
    "* Multiple outliers were identified in the price and freight_value but exactly they were the prices of the high-end/expensives products so we cannot remove it but in order to normalize it we transformed it using the logarithmic tranformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba6966",
   "metadata": {},
   "source": [
    "### Processing the `olist_products_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cce8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the products dataset\n",
    "products_dataset_path = os.path.join(path, \"olist_products_dataset.csv\")\n",
    "products_df = pd.read_csv(products_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a05351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the products_df DataFrame\n",
    "print(products_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in products_df:\")\n",
    "print(products_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df.rename(columns={\n",
    "    \"product_name_lenght\" : \"product_name_length\",\n",
    "    \"product_description_lenght\" : \"product_description_length\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3decb061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'product_category_name' with 'unknown'\n",
    "products_df['product_category_name'] = products_df['product_category_name'].fillna('unknown')\n",
    "\n",
    "# Verify that missing values in 'product_category_name' have been handled\n",
    "print(\"\\nMissing values in products_df after filling 'product_category_name':\")\n",
    "print(products_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median of 'product_name_length', 'product_description_length', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', and 'product_width_cm'\n",
    "median_name_length = products_df['product_name_length'].median()\n",
    "median_description_length = products_df['product_description_length'].median()\n",
    "median_photos_qty = products_df['product_photos_qty'].median()\n",
    "median_weight_g = products_df['product_weight_g'].median()\n",
    "median_length_cm = products_df['product_length_cm'].median()\n",
    "median_height_cm = products_df['product_height_cm'].median()\n",
    "median_width_cm = products_df['product_width_cm'].median()\n",
    "\n",
    "\n",
    "# Impute missing values in 'product_name_length', 'product_description_length', and 'product_photos_qty' with their medians\n",
    "products_df['product_name_length'] = products_df['product_name_length'].fillna(median_name_length)\n",
    "products_df['product_description_length'] = products_df['product_description_length'].fillna(median_description_length)\n",
    "products_df['product_photos_qty'] = products_df['product_photos_qty'].fillna(median_photos_qty)\n",
    "\n",
    "# Impute missing values in 'product_weight_g', 'product_length_cm', 'product_height_cm', and 'product_width_cm' with their medians\n",
    "products_df['product_weight_g'] = products_df['product_weight_g'].fillna(median_weight_g)\n",
    "products_df['product_length_cm'] = products_df['product_length_cm'].fillna(median_length_cm)\n",
    "products_df['product_height_cm'] = products_df['product_height_cm'].fillna(median_height_cm)\n",
    "products_df['product_width_cm'] = products_df['product_width_cm'].fillna(median_width_cm)\n",
    "\n",
    "\n",
    "# Verify that missing values in 'product_name_length' have been handled\n",
    "print(\"\\nMissing values in products_df after imputing numerical columns:\")\n",
    "print(products_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated Values\n",
    "print(f\"Duplicated values in order_reviews_df: {products_df.duplicated().sum()}\")\n",
    "print(f\"Duplicated values in order_reviews_df['product_id']: {products_df['product_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a532845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in order_items_df\n",
    "numerical_cols_items = products_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in order_items_df:\", numerical_cols_items)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_items:\n",
    "    detect_and_plot_outliers_iqr(products_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9175f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df['product_weight_g_log'] = np.log1p(products_df['product_weight_g'])\n",
    "\n",
    "# Optional but recommended: Create and transform volume\n",
    "products_df['product_volume_cm3'] = products_df['product_length_cm'] * products_df['product_height_cm'] * products_df['product_width_cm']\n",
    "products_df['product_volume_cm3_log'] = np.log1p(products_df['product_volume_cm3'])\n",
    "\n",
    "print(\"Log-transformed columns for weight and volume have been created.\")\n",
    "\n",
    "# --- 2. Cap the Product Description Length ---\n",
    "# Calculate the 99th percentile\n",
    "desc_len_cap = products_df['product_description_length'].quantile(0.99)\n",
    "print(f\"Product description length will be capped at: {desc_len_cap:.0f} characters.\")\n",
    "\n",
    "# Create a new capped column\n",
    "products_df['product_description_length_capped'] = products_df['product_description_length'].clip(upper=desc_len_cap)\n",
    "\n",
    "print(\"Capped column for description length has been created.\")\n",
    "\n",
    "# --- 3. Do Nothing for Photos Qty and Name Length ---\n",
    "print(\"No changes made to 'product_photos_qty' or 'product_name_length'.\")\n",
    "\n",
    "# Display the new columns\n",
    "print(\"\\nDataFrame with new transformed/capped columns:\")\n",
    "display(products_df[['product_weight_g', 'product_weight_g_log', 'product_volume_cm3', 'product_volume_cm3_log', 'product_description_length', 'product_description_length_capped']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc69be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df.to_parquet(\"olist_products_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916e98c",
   "metadata": {},
   "source": [
    "Observations\n",
    "* The dataset includes product identifiers, product categories (in Portuguese), and physical attributes such as length, height, width, and weight.\n",
    "* Around 1.85% of entries have missing values in category and product description-related columns, should be addressed by filling it with either unknown or with median values.\n",
    "* Only a negligible number of missing values are present in the physical dimension columns.\n",
    "* Some products share the same category ID, which will be translated into English using the product_category dataset.\n",
    "* Product dimensions and weight will be valuable for analyzing shipping costs and understanding product characteristics in later EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dbfb7f",
   "metadata": {},
   "source": [
    "### Processing the `olist_geolocation_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geolocation dataset\n",
    "geolocation_dataset_path = os.path.join(path, \"olist_geolocation_dataset.csv\")\n",
    "geolocation_df = pd.read_csv(geolocation_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "geolocation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the geolocation_df DataFrame\n",
    "geolocation_df_info = geolocation_df.info()\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in geolocation_df:\")\n",
    "print(geolocation_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49badbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows in the geolocation_df\n",
    "print(f\"Number of duplicated rows in geolocation_df: {geolocation_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27453b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows based on the subset of specified columns\n",
    "geolocation_df.drop_duplicates(subset=['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng'], keep='first', inplace=True)\n",
    "\n",
    "# Verify that duplicates based on the subset have been removed\n",
    "print(f\"Number of duplicated rows based on zip code, lat, and lng after removing duplicates: {geolocation_df.duplicated(subset=['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in order_items_df\n",
    "numerical_cols_items = geolocation_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in order_items_df:\", numerical_cols_items)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_items:\n",
    "    detect_and_plot_outliers_iqr(geolocation_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b983928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the approximate geographical boundaries for Brazil\n",
    "LAT_MIN, LAT_MAX = -34, 6\n",
    "LON_MIN, LON_MAX = -74, -34\n",
    "\n",
    "# Find coordinates that fall outside these boundaries\n",
    "geo_outliers = geolocation_df[\n",
    "    (geolocation_df['geolocation_lat'] < LAT_MIN) | (geolocation_df['geolocation_lat'] > LAT_MAX) |\n",
    "    (geolocation_df['geolocation_lng'] < LON_MIN) | (geolocation_df['geolocation_lng'] > LON_MAX)\n",
    "]\n",
    "\n",
    "num_outliers = len(geo_outliers)\n",
    "\n",
    "if num_outliers > 0:\n",
    "    print(f\"--- Geographic Outlier Analysis ---\")\n",
    "    print(f\"Found {num_outliers} coordinates outside the plausible boundaries of Brazil.\")\n",
    "    print(\"\\nDisplaying some of the detected outliers:\")\n",
    "    display(geo_outliers)\n",
    "\n",
    "    geolocation_df_cleaned = geolocation_df.drop(geo_outliers.index)\n",
    "    print(f\"\\n{num_outliers} outlier rows have been removed.\")\n",
    "\n",
    "else:\n",
    "    print(\"No geographic outliers found outside the plausible boundaries of Brazil.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation_df.to_parquet(\"olist_geolocation_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f5382",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Each zip code prefix is associated with one city and one state.\n",
    "* This dataset enables mapping customer and seller locations geographically.\n",
    "* Unique states provide insights into regional coverage and potential logistics challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260b463",
   "metadata": {},
   "source": [
    "### Processing the `product_category_name_translation_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the product category name translation dataset\n",
    "category_name_translation_dataset_path = os.path.join(path, \"product_category_name_translation.csv\")\n",
    "category_name_translation_df = pd.read_csv(category_name_translation_dataset_path)\n",
    "\n",
    "# Display the entire DataFrame\n",
    "category_name_translation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1412f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the category_name_translation_df DataFrame\n",
    "print(category_name_translation_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in category_name_translation_df:\")\n",
    "print(category_name_translation_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicated values in category_name_translation_df: {category_name_translation_df.duplicated().sum()}\")\n",
    "print(f\"Duplicated values in category_name_translation_df['product_category_name']: {category_name_translation_df['product_category_name'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f896194",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name_translation_df.to_parquet(\"category_name_translation_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185333d4",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* The dataset contains two columns: the original Portuguese category names and their English translations.\n",
    "* There are no missing values and duplicate values.\n",
    "* This will allow us to join with the products dataset to translate product categories for better readability in analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cdfb5a",
   "metadata": {},
   "source": [
    "### Processing the `olist_orders_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the orders dataset\n",
    "orders_dataset_path = os.path.join(path, \"olist_orders_dataset.csv\")\n",
    "orders_df = pd.read_csv(orders_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8590d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the orders_df DataFrame\n",
    "print(orders_df.info())\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in orders_df:\")\n",
    "print(orders_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb644d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\",\n",
    "    \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\"\n",
    "]\n",
    "\n",
    "for col in date_cols:\n",
    "    orders_df[col] = pd.to_datetime(orders_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where 'order_status' is 'delivered' and overwrite the original DataFrame\n",
    "orders_df = orders_df[orders_df['order_status'] == 'delivered']\n",
    "\n",
    "# Now, drop rows with missing delivery dates directly from 'orders_df'\n",
    "orders_df.dropna(subset=['order_delivered_customer_date'], inplace=True)\n",
    "\n",
    "print(\"Original DataFrame has been modified. âœ…\")\n",
    "print(\"\\nShape after filtering and dropping nulls:\", orders_df.shape)\n",
    "print(\"\\nRemaining missing values:\")\n",
    "print(orders_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97443e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.dropna(subset=['order_approved_at', 'order_delivered_carrier_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0334e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conditions for invalid orders\n",
    "invalid_order_conditions = (\n",
    "    (orders_df['order_approved_at'] < orders_df['order_purchase_timestamp']) |\n",
    "    (orders_df['order_delivered_carrier_date'] < orders_df['order_approved_at']) |\n",
    "    (orders_df['order_delivered_customer_date'] < orders_df['order_delivered_carrier_date']) |\n",
    "    (orders_df['order_estimated_delivery_date'] < orders_df['order_purchase_timestamp'])\n",
    ")\n",
    "\n",
    "# Filter out the invalid orders\n",
    "orders_df = orders_df[~invalid_order_conditions].reset_index(drop=True)\n",
    "\n",
    "# Display the number of remaining orders\n",
    "print(f\"Number of orders after removing invalid ones: {len(orders_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ade39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicated values in category_name_translation_df: {orders_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc832d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.to_parquet(\"olist_orders_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbf576",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Missing Values: Missing values were present in `order_approved_at`, `order_delivered_carrier_date`, and `order_delivered_customer_date`. These were not explicitly filled but orders with illogical timestamp sequences were removed.\n",
    "* Data Types: All timestamp columns (`order_purchase_timestamp`, `order_approved_at`, `order_delivered_carrier_date`, `order_delivered_customer_date`, `order_estimated_delivery_date`) were converted to datetime objects.\n",
    "* Duplicate Values: Checked for duplicate rows and duplicate `order_id` values, none were found.\n",
    "* Outliers: No numerical columns were present in this dataset for standard outlier detection using the IQR method on numerical values. However, orders with illogical timestamp sequences (e.g., delivery before purchase) were considered data quality issues and remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed626b0",
   "metadata": {},
   "source": [
    "### Processing the `olist_order_payments_dataset.csv`, this includes checking for null/missing values, ensuring all datatypes are correctly assigned to the columns, checking for duplicate values and checking for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the order payments dataset\n",
    "order_payments_dataset_path = os.path.join(path, \"olist_order_payments_dataset.csv\")\n",
    "order_payments_df = pd.read_csv(order_payments_dataset_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "order_payments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the order_payments_df DataFrame\n",
    "order_payments_df_info = order_payments_df.info()\n",
    "\n",
    "# Display missing values information\n",
    "print(\"\\nMissing values in order_payments_df:\")\n",
    "print(order_payments_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe34028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns in order_items_df\n",
    "numerical_cols_items = order_payments_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Numerical columns in order_items_df:\", numerical_cols_items)\n",
    "\n",
    "# Apply outlier detection and plotting for each numerical column\n",
    "for col in numerical_cols_items:\n",
    "    detect_and_plot_outliers_iqr(order_payments_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets to get full order context\n",
    "payments_and_orders_df = pd.merge(order_payments_df, orders_df, on='order_id')\n",
    "full_order_details_df = pd.merge(payments_and_orders_df, order_items_df, on='order_id')\n",
    "\n",
    "# Sort by payment_value to see the largest transactions\n",
    "top_payments = full_order_details_df.sort_values(by='payment_value', ascending=False)\n",
    "\n",
    "# Display the top 10 largest payments and their details\n",
    "print(\"\\nTop 10 Largest Transactions:\")\n",
    "print(top_payments.head(10)[['order_id', 'payment_value', 'price', 'freight_value', 'product_id']])\n",
    "\n",
    "# Investigate zero-value payments\n",
    "zero_value_payments = order_payments_df[order_payments_df['payment_value'] == 0]\n",
    "print(f\"\\nNumber of zero-value payments: {len(zero_value_payments)}\")\n",
    "print(\"Payment types for zero-value transactions:\")\n",
    "print(zero_value_payments['payment_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89be65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the 99.9th percentile\n",
    "cap_value = order_payments_df['payment_value'].quantile(0.999)\n",
    "print(f\"\\n99.9th percentile value (capping threshold): {cap_value:.2f}\")\n",
    "\n",
    "# Create a new column with the capped values\n",
    "order_payments_df['capped_payment_value'] = order_payments_df['payment_value'].clip(upper=cap_value)\n",
    "\n",
    "# Compare the original and capped statistics\n",
    "print(\"\\nStatistics after capping:\")\n",
    "print(order_payments_df[['payment_value', 'capped_payment_value']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_payments_df.to_parquet(\"olist_order_payments_cleaned_dataset.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace60444",
   "metadata": {},
   "source": [
    " Observation:\n",
    "* Missing Values: Missing values in `product_category_name` were filled with 'unknown'. Missing values in numerical columns (`product_name_length`, `product_description_length`, `product_photos_qty`, `product_weight_g`, `product_length_cm`, `product_height_cm`, `product_width_cm`) were imputed with their respective medians.\n",
    "* Data Types: Data types were checked and found to be appropriate after imputation.\n",
    "* Duplicate Values: Checked for duplicate rows and duplicate `product_id` values, none were found.\n",
    "* Outliers: Outliers were detected in several numerical columns. Logarithmic transformations (`np.log1p`) were applied to `product_weight_g` and `product_volume_cm3` to reduce the impact of outliers and normalize distributions. The `product_description_length` was capped at the 99th percentile."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
